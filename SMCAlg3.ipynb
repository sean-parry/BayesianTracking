{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# PlGreen Maskell 2017\n",
    "implementing the third algorithm second is just a simplificaiton of the first that uses less memory so may be useful for barkla but is kind of trivial at the smae time\n",
    "\n",
    "* this extension uses smc for the resampling alg as well, by the sounds of it\n",
    "* still on static param estimaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%pip install numpy\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nok so the reason it doesn't work is because its getting stuck in\\na resampling loop, idk how though - main estimate must be that the \\nweight calc is somehow off bc if you only take through ur good \\nsamples in the resampling event at the top of the loop\\nthen it shouldn't get stuck in a loop\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "beta_0 = 1\n",
    "beta_e = 100\n",
    "mu_0 = 0\n",
    "\n",
    "def normal_dist(x,mu,var):\n",
    "    sigma = var**(1/2)\n",
    "    return (1/(sigma*(np.pi*2)**(1/2)))*np.e**(-0.5*((x-mu)/sigma)**2)\n",
    "\n",
    "def yt_func(x_t:float,theta) -> float:\n",
    "    \"\"\"give the value of theta we want to check against the\n",
    "    observaiton\"\"\"\n",
    "    return theta*x_t\n",
    "\n",
    "def zt_func(x_t:float) -> float:\n",
    "    actual_theta = 1 # given becuase we are making observations\n",
    "    # of the real system\n",
    "    error = np.random.randn(1)[0]*(1/beta_e)\n",
    "    return yt_func(x_t,actual_theta)+error\n",
    "\n",
    "def prob_z_given_theta(z_1_to_t, x_1_to_t,theta):\n",
    "    \"\"\"\n",
    "    take arr z,x and scalar theta\n",
    "    \"\"\"\n",
    "    k = 1\n",
    "    sig_sum = 0\n",
    "    for zt, xt in zip(z_1_to_t,x_1_to_t):\n",
    "        sig_sum += (zt-yt_func(xt,theta))**2\n",
    "    return k*np.e**((-beta_e/2)*sig_sum)\n",
    "\n",
    "def prob_theta(theta):\n",
    "    return np.e**((-beta_0/2)*(theta-mu_0)**2)\n",
    "\n",
    "def beta_func(x_1_to_t):\n",
    "    sig_sum = 0\n",
    "    for xt in x_1_to_t:\n",
    "        sig_sum += xt**2\n",
    "    return beta_e*sig_sum+beta_0\n",
    "\n",
    "def mu_func(z_1_to_t,x_1_to_t):\n",
    "    sig_sum = 0\n",
    "    for zt, xt in zip(z_1_to_t,x_1_to_t):\n",
    "        sig_sum += xt*zt\n",
    "    beta = beta_func(x_1_to_t)\n",
    "    return (1/beta)*(beta_e*sig_sum+beta_0*mu_0)\n",
    "\n",
    "def prob_theta_given_z(theta,z_1_to_t,x_1_to_t):\n",
    "    return np.e**((-beta_func(x_1_to_t)/2)*theta-mu_func(z_1_to_t,x_1_to_t))\n",
    "\n",
    "# just some wrapper functions might help my understanding\n",
    "def liklihood(z_1_to_t,x_1_to_t, theta_arr):\n",
    "    return [prob_z_given_theta(z_1_to_t,x_1_to_t,theta) for theta in theta_arr]\n",
    "\n",
    "def prior(theta):\n",
    "    return prob_theta(theta)\n",
    "\n",
    "# im really not sure about this function\n",
    "# this being wrong does also make my l kernel wrong\n",
    "def prior_conditional(theta_hat,theta):\n",
    "    return normal_dist(theta_hat,theta,0.1)\n",
    "\n",
    "# since we are using gaussian proposal dist we acc don't need this\n",
    "# since this and the term above return identical p so cancel\n",
    "def l_kernel(theta, theta_hat):\n",
    "    return  normal_dist(theta,theta_hat,0.1)\n",
    "\n",
    "def get_inital_samples(n):\n",
    "    theta_arr = [mu_0]*n\n",
    "    return theta_arr+(np.random.randn(n)*beta_0)\n",
    "\n",
    "# idk is this is the correct word as techincally\n",
    "# this is the prior once the new timestep is reached\n",
    "# and we want to get the next posterior \n",
    "# which again becomes the new prior? idek\n",
    "def defensive_sampling(theta_arr):\n",
    "    probs = np.random.rand(len(theta_arr))\n",
    "    random_step = np.random.randn(len(theta_arr))\n",
    "    theta_hat = []\n",
    "    for th, p, r in zip(theta_arr,probs, random_step):\n",
    "        if p<=0.9:\n",
    "            theta_hat.append(th+r*0.1)\n",
    "        else:\n",
    "            theta_hat.append(th+r)\n",
    "    return np.array(theta_hat)     \n",
    "\n",
    "def posterior(z_1_to_t,x_1_to_t):\n",
    "    return prob_theta_given_z(z_1_to_t,x_1_to_t)\n",
    "\n",
    "def norm_weights(weights):\n",
    "    return weights/sum(weights)\n",
    "\n",
    "def reset_weights(n):\n",
    "    return np.array([1]*n)\n",
    "\n",
    "# this should just take zt the whole liklihood thing is probs \n",
    "# wrong\n",
    "def new_weights(weights,z_arr,x_arr,theta,theta_hat):\n",
    "    w_hat = []\n",
    "    for w,th,th_hat in zip(weights,theta,theta_hat):\n",
    "        temp1 = prob_z_given_theta(z_arr,x_arr,th_hat)\n",
    "        temp2 = prob_z_given_theta(z_arr,x_arr,th)\n",
    "        temp3 = temp1/temp2\n",
    "        #print(f't1:{temp1}:t2:{temp2}:t3:{temp3}')\n",
    "        # just getting rid of infs and nans so 0 val for all of \n",
    "        # them shouldn't matter after i normalise although\n",
    "        # probably affects the efficiency\n",
    "        # technically temp2 will always be one given the l kernel we chose\n",
    "        # so just setting temp2 = 1\n",
    "        temp4 = l_kernel(th, th_hat)/prior_conditional(th_hat,th)\n",
    "        w_hat.append(w*temp3*temp4)\n",
    "    return w_hat\n",
    "\n",
    "def get_neff(weights):\n",
    "    squared_sum = 0\n",
    "    for w in weights:\n",
    "        squared_sum += w*w\n",
    "    return 1/squared_sum\n",
    "\n",
    "#importance sampling\n",
    "def resample(curr_theta, weights):\n",
    "    new_theta = []\n",
    "    new_theta = np.random.choice(curr_theta,size = len(curr_theta), p=weights)\n",
    "    return np.array(new_theta)\n",
    "\n",
    "\n",
    "def prob_z_scalar_given_theta(zt, xt,theta):\n",
    "    \"\"\"\n",
    "    take scalar z,x and scalar theta\n",
    "    \"\"\"\n",
    "    k = 1\n",
    "    sig_sum = (zt-yt_func(xt,theta))**2\n",
    "    return k*np.e**((-beta_e/2)*sig_sum)\n",
    "\n",
    "def new_weights_different(weights, z,x,theta_arr):\n",
    "    new_weights = []\n",
    "    for th,w in zip(theta_arr,weights):\n",
    "        new_weights.append(w*prob_z_scalar_given_theta(z,x,th))\n",
    "    return new_weights\n",
    "\n",
    "total_t = 500\n",
    "# remember the observations happen sequentially\n",
    "xt_arr = []\n",
    "xt_arr.append(np.random.rand(1)[0])\n",
    "# grab all of yt bear in mind i shouldn't know this it will\n",
    "# only be needed for plotting later\n",
    "# yt_arr = np.array([yt_func(x) for x in xt_arr])\n",
    "\n",
    "zt_arr = []\n",
    "n = 100\n",
    "\n",
    "theta = get_inital_samples(n=n)\n",
    "zt_arr.append(zt_func(xt_arr[-1]))\n",
    "weights = liklihood(zt_arr,xt_arr,theta)\n",
    "\n",
    "time = 1\n",
    "while time<total_t:\n",
    "    # new system change\n",
    "    xt_arr.append(np.random.rand(1)[0])\n",
    "    # new observation is then made\n",
    "    zt_arr.append(zt_func(xt_arr[-1]))\n",
    "    weights = norm_weights(weights)\n",
    "    neff = get_neff(weights)\n",
    "    while (neff<n/2):\n",
    "        theta = resample(theta,weights)\n",
    "        weights = reset_weights(n)\n",
    "        theta_hat = defensive_sampling(theta)\n",
    "        weights = new_weights(weights,zt_arr,xt_arr,theta,theta_hat)\n",
    "        theta = theta_hat\n",
    "        weights = norm_weights(weights)\n",
    "        neff = get_neff(weights)\n",
    "        # temp break so it doesn't get stuck in a loop just\n",
    "        break\n",
    "    time+=1\n",
    "    weights = new_weights_different(weights,zt_arr[-1],xt_arr[1],theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "* ok so the reason it doesn't work is because its getting stuck in\n",
    "a resampling loop, idk how though - my best guess is that the \n",
    "weight calc is somehow off bc if you only take through ur good \n",
    "samples in the resampling event at the top of the loop\n",
    "then it shouldn't get stuck in a loop\n",
    "\n",
    "* If i Break the loop so that it doesn't get stuck the code still\n",
    "produces a really good estimate for theta, i still think the new weights function is wrong though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(8.504420984997471e-07), np.float64(1.4103300642260759e-63), np.float64(1.5641580476010656e-16), np.float64(1.5948441573943547e-135), np.float64(6.9144818261456816e-15), np.float64(6.255674745162099e-12), np.float64(9.677140635465129e-48), np.float64(1.4313979487297423e-18), np.float64(1.7439397883217697e-08), np.float64(1.8190883944923794e-25)]\n",
      "this should be close to one it is: 0.9957128912094595\n"
     ]
    }
   ],
   "source": [
    "print(weights[:10])\n",
    "theta_pred = np.linalg.matmul(norm_weights(weights),theta)\n",
    "\n",
    "print(f'this should be close to one it is: {theta_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new weights\n",
    "\n",
    "so theta describes the dynamics of the system it just so happens that this system is just linear, so theta is 1d and should be found to equal one, so when we change theta we check it against all previous observations because the funciton/equation isn't dependant on time?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
