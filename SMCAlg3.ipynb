{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# PlGreen Maskell 2017\n",
    "implementing the third algorithm second is just a simplificaiton of the first that uses less memory so may be useful for barkla but is kind of trivial at the smae time\n",
    "\n",
    "* this extension uses smc for the resampling alg as well, by the sounds of it\n",
    "* still on static param estimaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%pip install numpy\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.9673323511640602), np.float64(1.02748760930024), np.float64(0.0001473644221641204), np.float64(0.921839317261762), np.float64(0.9933868590694457), np.float64(0.5991300798561243), np.float64(0.925097661658791), np.float64(0.976958195297787), np.float64(0.07096544439037007), np.float64(0.4617821056967126)]\n",
      "[np.float64(0.22188224992477898), np.float64(10.34753695097968), np.float64(0.07663118585156684), np.float64(251.64984754630657), np.float64(0.7872248953295258), np.float64(32.28083210954644), np.float64(0.11142360131923208), np.float64(19911.846888708307), np.float64(0.15405300292310817), np.float64(7.7282731681135495)]\n",
      "[np.float64(0.0001030596468510544), np.float64(8.508901687539302), np.float64(0.020345862640781958), np.float64(1.2891811960904566e-06), np.float64(4.765369606312258e-10), np.float64(34.60264925693916), np.float64(3.1250696085220194e-06), np.float64(0.0003364853518269944), np.float64(1.1740539034588866e-09), np.float64(24.04846471921546)]\n",
      "[np.float64(0.03331227961480996), np.float64(3995.898919782064), np.float64(7.185111765937881), np.float64(0.10289129575218439), np.float64(0.028490992030550204), np.float64(6.135038803390985e-06), np.float64(0.02876690621037982), np.float64(133.96673117584018), np.float64(148.32896153824106), np.float64(21.595962846209705)]\n",
      "[np.float64(57.56244836242368), np.float64(31.281908577446085), np.float64(0.26176613881614447), np.float64(3.9727944889727995), np.float64(2.1159329907099615), np.float64(3.2481242396511663), np.float64(7.586499780137178), np.float64(2.278502387188977), np.float64(1.8214863054539658), np.float64(1.761770732842268)]\n",
      "[np.float64(0.8492437746729136), np.float64(0.17097835478271434), np.float64(0.25708745654988296), np.float64(0.5726994473520706), np.float64(0.12231172038033242), np.float64(3.367710601141234), np.float64(0.9480993784421484), np.float64(8.632362802280724e-51), np.float64(1.0050667637311852), np.float64(0.13079809639633178)]\n",
      "[np.float64(1.041815019479723), np.float64(0.9661266325274916), np.float64(0.9936331581615612), np.float64(0.9147019316922106), np.float64(0.7489375941567636), np.float64(1.059481460393243), np.float64(1.0152633109235976e-07), np.float64(1.9204268685830437), np.float64(0.000365184802728126), np.float64(0.25075002032065946)]\n",
      "[np.float64(1.3964005078657898e-06), np.float64(0.41413550492984286), np.float64(1.2698875901350957), np.float64(0.7957441428962392), np.float64(0.5353196645880677), np.float64(0.8348880740040081), np.float64(0.07731710429632424), np.float64(1.826198088978), np.float64(0.73305841892428), np.float64(0.9323734968580337)]\n",
      "[np.float64(4.483313484027825), np.float64(5.099512999010407e-07), np.float64(0.06833398045130798), np.float64(3.043065772714344), np.float64(2.3333771832783156e-09), np.float64(0.01678816886252417), np.float64(0.694198491154872), np.float64(2.11365995711234), np.float64(2.113198894127577), np.float64(5.84754837771305e-20)]\n",
      "[np.float64(0.06590489716976727), np.float64(0.8930064608499437), np.float64(0.22156620371291366), np.float64(7.4919988341583676e-93), np.float64(0.4532823664535083), np.float64(2.2285053019930774), np.float64(0.4234464976061502), np.float64(0.0014438718673208876), np.float64(0.03798242980700411), np.float64(0.37956383775677627)]\n",
      "[np.float64(0.2139813183882703), np.float64(0.15734514511009462), np.float64(0.32571465477334993), np.float64(4.060855519254401), np.float64(0.9894658842878523), np.float64(2.7575322905848715e-32), np.float64(1.0093071804744966), np.float64(0.9939194735027048), np.float64(0.042234381577072044), np.float64(0.17369934526468808)]\n",
      "[np.float64(5.854781783465191), np.float64(2.665975209672326), np.float64(0.0005505305427492529), np.float64(0.21686004300429884), np.float64(0.00031654490451686954), np.float64(0.3657065696688996), np.float64(0.20786876867916412), np.float64(0.6522093823454426), np.float64(0.12134622449974299), np.float64(0.0033081464222299004)]\n",
      "[np.float64(0.09842169167996172), np.float64(21.158074614530257), np.float64(0.0011686615172283032), np.float64(1.051486312415096), np.float64(0.6991489972480508), np.float64(0.7028496238977026), np.float64(0.019319752816077895), np.float64(0.16487670663400766), np.float64(0.4327542995865462), np.float64(0.4917726140134031)]\n",
      "[np.float64(1.0881385253360611), np.float64(0.14735003125340979), np.float64(1.0772355503689086), np.float64(0.27502592196523745), np.float64(0.7936070647601836), np.float64(0.8045387404551363), np.float64(0.07329285160614683), np.float64(0.09147586450621373), np.float64(1.4549350966057312e-89), np.float64(0.009154457184474392)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "beta_0 = 1\n",
    "beta_e = 100\n",
    "mu_0 = 0\n",
    "\n",
    "def normal_dist(x,mu,var):\n",
    "    sigma = var**(1/2)\n",
    "    return (1/(sigma*(np.pi*2)**(1/2)))*np.e**(-0.5*((x-mu)/sigma)**2)\n",
    "\n",
    "def yt_func(x_t:float,theta) -> float:\n",
    "    \"\"\"give the value of theta we want to check against the\n",
    "    observaiton\"\"\"\n",
    "    return theta*x_t\n",
    "\n",
    "def zt_func(x_t:float) -> float:\n",
    "    actual_theta = 1 # given becuase we are making observations\n",
    "    # of the real system\n",
    "    error = np.random.randn(1)[0]*(1/beta_e)\n",
    "    return yt_func(x_t,actual_theta)+error\n",
    "\n",
    "def prob_z_given_theta(z_1_to_t, x_1_to_t,theta):\n",
    "    \"\"\"\n",
    "    take arr z,x and scalar theta\n",
    "    \"\"\"\n",
    "    k = 1\n",
    "    sig_sum = 0\n",
    "    for zt, xt in zip(z_1_to_t,x_1_to_t):\n",
    "        sig_sum += (zt-yt_func(xt,theta))**2\n",
    "    return k*np.e**((-beta_e/2)*sig_sum)\n",
    "\n",
    "def prob_theta(theta):\n",
    "    return np.e**((-beta_0/2)*(theta-mu_0)**2)\n",
    "\n",
    "def beta_func(x_1_to_t):\n",
    "    sig_sum = 0\n",
    "    for xt in x_1_to_t:\n",
    "        sig_sum += xt**2\n",
    "    return beta_e*sig_sum+beta_0\n",
    "\n",
    "def mu_func(z_1_to_t,x_1_to_t):\n",
    "    sig_sum = 0\n",
    "    for zt, xt in zip(z_1_to_t,x_1_to_t):\n",
    "        sig_sum += xt*zt\n",
    "    beta = beta_func(x_1_to_t)\n",
    "    return (1/beta)*(beta_e*sig_sum+beta_0*mu_0)\n",
    "\n",
    "def prob_theta_given_z(theta,z_1_to_t,x_1_to_t):\n",
    "    return np.e**((-beta_func(x_1_to_t)/2)*theta-mu_func(z_1_to_t,x_1_to_t))\n",
    "\n",
    "# just some wrapper functions might help my understanding\n",
    "def liklihood(z_1_to_t,x_1_to_t, theta_arr):\n",
    "    return [prob_z_given_theta(z_1_to_t,x_1_to_t,theta) for theta in theta_arr]\n",
    "\n",
    "def prior(theta):\n",
    "    return prob_theta(theta)\n",
    "\n",
    "# im really not sure about this function\n",
    "# this being wrong does also make my l kernel wrong\n",
    "def prior_conditional(theta_hat,theta):\n",
    "    return normal_dist(theta_hat,theta,0.1)\n",
    "\n",
    "# since we are using gaussian proposal dist we acc don't need this\n",
    "# since this and the term above return identical p so cancel\n",
    "def l_kernel(theta, theta_hat):\n",
    "    return  normal_dist(theta,theta_hat,0.1)\n",
    "\n",
    "def get_inital_samples(n):\n",
    "    theta_arr = [mu_0]*n\n",
    "    return theta_arr+(np.random.randn(n)*beta_0)\n",
    "\n",
    "# idk is this is the correct word as techincally\n",
    "# this is the prior once the new timestep is reached\n",
    "# and we want to get the next posterior \n",
    "# which again becomes the new prior? idek\n",
    "def defensive_sampling(theta_arr):\n",
    "    probs = np.random.rand(len(theta_arr))\n",
    "    random_step = np.random.randn(len(theta_arr))\n",
    "    theta_hat = []\n",
    "    for th, p, r in zip(theta_arr,probs, random_step):\n",
    "        if p<=0.9:\n",
    "            theta_hat.append(th+r*0.1)\n",
    "        else:\n",
    "            theta_hat.append(th+r)\n",
    "    return np.array(theta_hat)     \n",
    "\n",
    "def posterior(z_1_to_t,x_1_to_t):\n",
    "    return prob_theta_given_z(z_1_to_t,x_1_to_t)\n",
    "\n",
    "def norm_weights(weights):\n",
    "    return weights/sum(weights)\n",
    "\n",
    "def reset_weights(n):\n",
    "    return np.array([1]*n)\n",
    "\n",
    "# this should just take zt the whole liklihood thing is probs \n",
    "# wrong\n",
    "def new_weights(weights,z_arr,x_arr,theta,theta_hat):\n",
    "    w_hat = []\n",
    "    for w,th,th_hat in zip(weights,theta,theta_hat):\n",
    "        temp1 = prob_z_given_theta(z_arr,x_arr,th_hat)\n",
    "        temp2 = prob_z_given_theta(z_arr,x_arr,th)\n",
    "        temp3 = temp1/temp2\n",
    "        #print(f't1:{temp1}:t2:{temp2}:t3:{temp3}')\n",
    "        # just getting rid of infs and nans so 0 val for all of \n",
    "        # them shouldn't matter after i normalise although\n",
    "        # probably affects the efficiency\n",
    "        # technically temp2 will always be one given the l kernel we chose\n",
    "        # so just setting temp2 = 1\n",
    "        temp4 = l_kernel(th, th_hat)/prior_conditional(th_hat,th)\n",
    "        w_hat.append(w*temp3*temp4)\n",
    "    return w_hat\n",
    "\n",
    "def get_neff(weights):\n",
    "    squared_sum = 0\n",
    "    for w in weights:\n",
    "        squared_sum += w*w\n",
    "    return 1/squared_sum\n",
    "\n",
    "#importance sampling\n",
    "def resample(curr_theta, weights):\n",
    "    new_theta = []\n",
    "    new_theta = np.random.choice(curr_theta,size = len(curr_theta), p=weights)\n",
    "    return np.array(new_theta)\n",
    "\n",
    "\n",
    "def prob_z_scalar_given_theta(zt, xt,theta):\n",
    "    \"\"\"\n",
    "    take scalar z,x and scalar theta\n",
    "    \"\"\"\n",
    "    k = 1\n",
    "    sig_sum = (zt-yt_func(xt,theta))**2\n",
    "    return k*np.e**((-beta_e/2)*sig_sum)\n",
    "\n",
    "def new_weights_different(weights, z,x,theta_arr):\n",
    "    new_weights = []\n",
    "    for th,w in zip(theta_arr,weights):\n",
    "        new_weights.append(w*prob_z_scalar_given_theta(z,x,th))\n",
    "    return new_weights\n",
    "\n",
    "total_t = 10\n",
    "# remember the observations happen sequentially\n",
    "xt_arr = []\n",
    "xt_arr.append(np.random.rand(1)[0])\n",
    "# grab all of yt bear in mind i shouldn't know this it will\n",
    "# only be needed for plotting later\n",
    "# yt_arr = np.array([yt_func(x) for x in xt_arr])\n",
    "\n",
    "zt_arr = []\n",
    "n = 10\n",
    "\n",
    "theta = get_inital_samples(n=n)\n",
    "zt_arr.append(zt_func(xt_arr[-1]))\n",
    "weights = liklihood(zt_arr,xt_arr,theta)\n",
    "\n",
    "time = 1\n",
    "while time<total_t:\n",
    "    # new system change\n",
    "    xt_arr.append(np.random.rand(1)[0])\n",
    "    # new observation is then made\n",
    "    zt_arr.append(zt_func(xt_arr[-1]))\n",
    "\n",
    "    weights = norm_weights(weights)\n",
    "    neff = get_neff(weights)\n",
    "    while (neff<n/2):\n",
    "        theta = resample(theta,weights)\n",
    "        weights = reset_weights(n)\n",
    "        theta_hat = defensive_sampling(theta)\n",
    "        weights = new_weights(weights,zt_arr,xt_arr,theta,theta_hat)\n",
    "        print(weights)\n",
    "        theta = theta_hat\n",
    "        weights = norm_weights(weights)\n",
    "        neff = get_neff(weights)\n",
    "    time+=1\n",
    "    weights = new_weights_different(weights,zt_arr[-1],xt_arr[1],theta)\n",
    "\n",
    "# new issue getting soo many small weights maybe the s.d is too high\n",
    "# so my terrible predictions are causeing issues\n",
    "\n",
    "# ok so the problem is that we use the prior default prior dist\n",
    "# every timestep we need to use the posterior dist for the prev\n",
    "# timestep which lowers the beta so the changes to theta slowly\n",
    "# get smaller as the observations increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.19835112957296502), np.float64(0.03373071871400874), np.float64(0.20043538662524407), np.float64(0.025259619127110344), np.float64(0.10821222924566401), np.float64(0.11045144724791786), np.float64(0.01651774189968093), np.float64(0.02074698439568548), np.float64(5.217451260636276e-106), np.float64(0.001867189113129835)]\n",
      "this should be close to one it is: 0.9907635058998161\n"
     ]
    }
   ],
   "source": [
    "print(weights[:10])\n",
    "theta_pred = np.linalg.matmul(norm_weights(weights),theta)\n",
    "\n",
    "print(f'this should be close to one it is: {theta_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new weights\n",
    "\n",
    "so theta describes the dynamics of the system it just so happens that this system is just linear, so theta is 1d and should be found to equal one, so when we change theta we check it against all previous observations because the funciton/equation isn't dependant on time?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
