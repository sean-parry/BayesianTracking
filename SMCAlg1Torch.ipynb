{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/GreenMaskAlg1.png)\n",
    "![images](images/GreenMaskAlg2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smc initialised, use run_smc to run all iterations\n",
      "[tensor(3.0225), tensor(4.5730), tensor(4.8642), tensor(4.9239), tensor(4.9408), tensor(4.9520), tensor(4.9452), tensor(4.8282), tensor(4.9852), tensor(5.0388), tensor(4.9631), tensor(4.8690), tensor(4.8601), tensor(4.7689), tensor(5.0031), tensor(4.8176), tensor(4.9664), tensor(4.9153), tensor(4.9050), tensor(4.8102), tensor(4.7860), tensor(4.7783), tensor(4.8916), tensor(5.0045), tensor(5.0449), tensor(4.9687), tensor(4.8562), tensor(5.0189), tensor(5.1008), tensor(5.1233), tensor(4.9787), tensor(4.9676), tensor(4.9212), tensor(5.1613), tensor(5.0879), tensor(4.9382), tensor(4.8678), tensor(4.7878), tensor(4.8079), tensor(5.0211), tensor(5.1208), tensor(4.9649), tensor(5.0070), tensor(5.0951), tensor(5.0293), tensor(5.1089), tensor(5.0958), tensor(5.0294), tensor(5.0230), tensor(5.0851), tensor(4.9760), tensor(4.9911), tensor(4.9758), tensor(5.0324), tensor(5.0074), tensor(5.0839), tensor(4.8563), tensor(4.9678), tensor(4.8290), tensor(5.4945), tensor(5.1499), tensor(5.2481), tensor(5.1131), tensor(5.0590), tensor(5.0212), tensor(4.9496), tensor(4.8760), tensor(4.8321), tensor(5.0113), tensor(4.8952), tensor(5.0054), tensor(4.8945), tensor(4.8646), tensor(5.0025), tensor(5.1568), tensor(4.8847), tensor(5.0690), tensor(5.1386), tensor(4.8797), tensor(5.1931), tensor(4.9668), tensor(5.0527), tensor(4.9016), tensor(5.1461), tensor(5.0107), tensor(5.0382), tensor(5.2152), tensor(5.0726), tensor(5.0521), tensor(5.0838), tensor(4.9196), tensor(4.9690), tensor(4.9032), tensor(4.9340), tensor(4.9949), tensor(4.9490), tensor(4.9613), tensor(5.0385), tensor(5.0608), tensor(5.1106), tensor(5.0509), tensor(5.0889), tensor(4.9943), tensor(5.0693), tensor(4.9823), tensor(4.9462), tensor(4.9991), tensor(5.0231), tensor(4.8579), tensor(5.0008), tensor(5.1135), tensor(5.0664), tensor(5.0515), tensor(5.0926), tensor(5.0562), tensor(4.9865), tensor(4.9469), tensor(5.0218), tensor(4.9524), tensor(4.9964), tensor(5.0986), tensor(4.8727), tensor(4.5939), tensor(4.9610), tensor(4.8634), tensor(4.8186), tensor(5.0051), tensor(5.0901), tensor(5.0902), tensor(5.0693), tensor(5.0366), tensor(5.0352), tensor(4.9152), tensor(5.0175), tensor(4.9637), tensor(5.0401), tensor(5.0040), tensor(5.1063), tensor(4.9486), tensor(5.2052), tensor(4.9989), tensor(5.1131), tensor(4.9974), tensor(4.9360), tensor(5.0693), tensor(4.9776), tensor(5.0533), tensor(5.0153), tensor(4.9218), tensor(4.9009), tensor(4.9553), tensor(4.9802), tensor(5.0135), tensor(4.9733), tensor(5.0687), tensor(5.6477), tensor(5.1059), tensor(5.0680), tensor(4.9971), tensor(5.0289), tensor(4.9334), tensor(4.9307), tensor(5.0705), tensor(5.0938), tensor(5.0824), tensor(5.0622), tensor(4.9285), tensor(5.0738), tensor(5.0760), tensor(5.0332), tensor(5.0673), tensor(5.3749), tensor(5.1251), tensor(4.9935), tensor(4.9925), tensor(5.0065), tensor(4.9261), tensor(4.9554), tensor(5.0712), tensor(4.9838), tensor(4.9939), tensor(5.0049), tensor(4.9989), tensor(4.9479), tensor(5.0881), tensor(5.1803), tensor(5.1797), tensor(5.1130), tensor(5.0212), tensor(5.0530), tensor(4.9577), tensor(4.9998), tensor(5.0354), tensor(4.9767), tensor(4.8842), tensor(5.0180), tensor(5.0036), tensor(5.2308), tensor(5.0900), tensor(5.0928), tensor(4.9013), tensor(5.0622), tensor(5.0312), tensor(5.0566), tensor(5.2502), tensor(4.8312), tensor(4.9909), tensor(5.0196), tensor(4.8793), tensor(4.9457), tensor(5.0135), tensor(5.0374), tensor(5.1980), tensor(4.9358), tensor(4.9967), tensor(5.0351), tensor(5.0588), tensor(5.0458), tensor(4.8754), tensor(4.8965), tensor(5.0624), tensor(5.1552), tensor(4.8399), tensor(4.9515), tensor(4.9101), tensor(4.9918), tensor(5.0155), tensor(5.0026), tensor(5.0448), tensor(4.9431), tensor(5.0406), tensor(4.9612), tensor(4.9694), tensor(5.0485), tensor(5.1260), tensor(5.1136), tensor(4.9704), tensor(4.9210), tensor(4.8565), tensor(4.8709), tensor(5.0598), tensor(5.0294), tensor(5.0961), tensor(5.1221), tensor(5.0640), tensor(4.9402), tensor(4.9931), tensor(4.9623), tensor(4.9392), tensor(5.0703), tensor(5.0594), tensor(5.2342), tensor(5.0755), tensor(4.9994), tensor(4.9644), tensor(4.9213), tensor(5.0361), tensor(4.7044), tensor(5.0054), tensor(4.9210), tensor(5.0086), tensor(4.9546), tensor(4.9620), tensor(4.9276), tensor(4.9985), tensor(4.9202), tensor(4.9621), tensor(5.1623), tensor(5.1906), tensor(5.0844), tensor(5.0580), tensor(5.0327), tensor(5.0919), tensor(5.0131), tensor(5.0266), tensor(5.0097), tensor(5.0305), tensor(4.8599), tensor(5.1850), tensor(4.9186), tensor(5.0385), tensor(5.1164), tensor(5.1348), tensor(4.9731), tensor(5.1280), tensor(5.0934), tensor(5.0858), tensor(5.1845), tensor(5.0201), tensor(4.8568), tensor(4.7418), tensor(4.9162), tensor(4.8752), tensor(4.9279), tensor(4.9371), tensor(4.8964), tensor(4.9389), tensor(4.9961), tensor(5.1118), tensor(5.1190), tensor(4.9261), tensor(4.9860), tensor(4.9607), tensor(4.9312), tensor(5.1838), tensor(5.1783), tensor(5.0087), tensor(5.0151), tensor(4.9488), tensor(5.0163), tensor(5.0447), tensor(4.9715), tensor(4.9889), tensor(4.9675), tensor(4.9918), tensor(4.9394), tensor(4.8348), tensor(5.0178), tensor(4.9296), tensor(5.0502), tensor(4.8976), tensor(4.9990), tensor(4.7254), tensor(4.9052), tensor(5.2855), tensor(5.0733), tensor(5.0932), tensor(4.9585), tensor(5.1220), tensor(5.0277), tensor(5.0868), tensor(5.0542), tensor(5.0603), tensor(5.0514), tensor(5.0032), tensor(4.9417), tensor(5.0500), tensor(5.1496), tensor(4.8122), tensor(4.9026), tensor(4.5351), tensor(4.8778), tensor(4.8729), tensor(4.9022), tensor(5.0178), tensor(4.9999), tensor(5.0263), tensor(5.0946), tensor(5.1035), tensor(5.0062), tensor(5.0017), tensor(5.1278), tensor(5.3128), tensor(5.2585), tensor(5.0875), tensor(5.2124), tensor(5.0708), tensor(5.0483), tensor(5.1087), tensor(5.0050), tensor(4.9821), tensor(5.1353), tensor(5.1508), tensor(5.1504), tensor(5.1803), tensor(5.0586), tensor(5.0017), tensor(4.9765), tensor(4.8194), tensor(4.9743), tensor(4.7786), tensor(4.9191), tensor(4.9326), tensor(4.9725), tensor(4.9870), tensor(5.0908), tensor(5.1221), tensor(5.0150), tensor(5.0441), tensor(4.9497), tensor(4.8255), tensor(5.0063), tensor(4.6180), tensor(4.9395), tensor(4.7653), tensor(4.9776), tensor(5.0058), tensor(5.0023), tensor(4.9906), tensor(5.0305), tensor(5.0550), tensor(5.0821), tensor(5.0624), tensor(5.1348), tensor(5.0369), tensor(5.1722), tensor(4.9548), tensor(5.1956), tensor(5.1753), tensor(5.0561), tensor(5.0602), tensor(5.1923), tensor(5.0652), tensor(5.0462), tensor(5.0331), tensor(4.9612), tensor(4.9087), tensor(4.9484), tensor(4.8238), tensor(5.0950), tensor(4.9174), tensor(5.0250), tensor(4.9731), tensor(5.0918), tensor(5.0841), tensor(5.0809), tensor(5.0153), tensor(5.0794), tensor(5.1645), tensor(5.1847), tensor(5.0046), tensor(5.0659), tensor(5.1041), tensor(5.1424), tensor(5.0297), tensor(4.9773), tensor(4.9455), tensor(5.1396), tensor(5.1324), tensor(5.0361), tensor(5.1358), tensor(5.1216), tensor(4.9902), tensor(5.0848), tensor(5.0583), tensor(4.9840), tensor(5.2042), tensor(5.0631), tensor(5.0155), tensor(5.0817), tensor(4.9573), tensor(5.0303), tensor(4.9524), tensor(4.9647), tensor(4.9637), tensor(5.0656), tensor(4.9801), tensor(4.8903), tensor(4.9737), tensor(4.9826), tensor(4.9045), tensor(4.9616), tensor(4.9998), tensor(5.0186), tensor(4.8893), tensor(4.9857), tensor(4.9286), tensor(4.8829), tensor(4.9561), tensor(4.5246), tensor(4.9550), tensor(5.1192), tensor(4.7711), tensor(5.0111), tensor(4.9434), tensor(4.9702), tensor(4.9583), tensor(4.9388), tensor(5.1405), tensor(5.0430), tensor(4.9257), tensor(4.8889), tensor(4.9518), tensor(5.0455), tensor(5.2361), tensor(5.0504), tensor(4.8600), tensor(5.0228), tensor(5.1142), tensor(4.8612), tensor(4.9525), tensor(4.9638), tensor(5.0225), tensor(5.0413), tensor(5.0430), tensor(5.0598), tensor(5.0615), tensor(5.0805), tensor(4.9416), tensor(4.9912), tensor(5.0709), tensor(5.0415), tensor(4.9724), tensor(5.0726), tensor(5.1007), tensor(5.0431), tensor(4.9432), tensor(5.2139), tensor(5.2697), tensor(5.0505)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "\n",
    "class SMC():\n",
    "    def __init__(self, n_threads: int, n_iterations: int):\n",
    "        self.n_threads = n_threads\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta = [self.initial_theta(loc=0, scale=1)]\n",
    "        self.weights = [self.initial_weights()]\n",
    "        self.estimate = []\n",
    "        print(f'smc initialised, use run_smc to run all iterations')\n",
    "        return\n",
    "    \n",
    "    def target_dist_prob(self, th):\n",
    "        normal = Normal(torch.tensor([5]),torch.tensor([0.5]))\n",
    "        return torch.exp(normal.log_prob(th))\n",
    "\n",
    "    def initial_weights(self):\n",
    "        normal = Normal(torch.tensor([0]),torch.tensor([1]))\n",
    "        # maybe this is stupid but idk why theres no way of getting prob\n",
    "        # only thing that i can get returned is log prob instead\n",
    "        prob_q = (torch.exp(normal.log_prob(self.theta[-1])))\n",
    "\n",
    "        prob_target = self.target_dist_prob(self.theta[-1])\n",
    "   \n",
    "        return prob_target/prob_q\n",
    "    \n",
    "    def initial_theta(self, loc, scale):\n",
    "        # maybe this is stupid but idk why theres no\n",
    "        return torch.normal(loc, scale,size=(1,self.n_threads))[-1]\n",
    "    \n",
    "    def normalise_weights_(self):\n",
    "        self.weights[-1] = self.weights[-1] / self.weights[-1].sum()\n",
    "    \n",
    "    def calc_neff(self):\n",
    "        return 1 / (self.weights[-1] @ self.weights[-1].T)\n",
    "    \n",
    "    def new_weights_(self):\n",
    "        target_new = self.target_dist_prob(self.theta[-1])\n",
    "        target_old = self.target_dist_prob(self.theta[-2])\n",
    "        self.weights.append(self.weights[-1] * (target_new/target_old))\n",
    "        return\n",
    "    \n",
    "    def weighted_resample_(self):\n",
    "        r\"\"\"\n",
    "        in place chages theta and weights using a weighted resample\n",
    "        \"\"\"\n",
    "        new_sample_indexes = torch.multinomial(self.weights[-1],self.n_threads,replacement=True)\n",
    "\n",
    "        theta_hat = torch.tensor([self.theta[-1][i] for i in new_sample_indexes])\n",
    " \n",
    "        self.theta[-1] = theta_hat\n",
    "        self.weights[-1] = (torch.ones(1,self.n_threads)[-1])*(1/self.n_threads)\n",
    "\n",
    "    def estimator_(self):\n",
    "        self.estimate.append(self.theta[-1] @ self.weights[-1].T)\n",
    "\n",
    "    def sample_(self):\n",
    "        self.theta.append(self.theta[-1]+torch.normal(0,1,size=(1,self.n_threads))[-1])\n",
    "\n",
    "    def run_smc(self):\n",
    "        for _ in range(self.n_iterations):\n",
    "            self.normalise_weights_()\n",
    "            # estimate quals of interest\n",
    "            self.estimator_()\n",
    "\n",
    "            # assess if we need to resample\n",
    "            neff = self.calc_neff()\n",
    "            if neff<(self.n_threads/2):\n",
    "                self.weighted_resample_()\n",
    "            self.sample_()\n",
    "            self.new_weights_()\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    n = 100\n",
    "    t = 500\n",
    "    smc_torch = SMC(n,t)\n",
    "    smc_torch.run_smc()\n",
    "    print(smc_torch.estimate)\n",
    "    return\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for large n its about 2-3 times slower than the numpy implementation, although idk how much the class stuff also slows it down, should make a general high dimensional case for both and run on my pc at home so as it acctaully has a gpu so should take advantage of cuda optimistaion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
