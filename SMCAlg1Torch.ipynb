{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images](images/GreenMaskAlg1.png)\n",
    "![images](images/GreenMaskAlg2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smc initialised, use run_smc to run all iterations\n",
      "[tensor(2.3095), tensor(4.5821), tensor(4.8225), tensor(4.9805), tensor(5.0322), tensor(4.9884), tensor(5.1005), tensor(4.9636), tensor(5.0626), tensor(5.0666), tensor(5.1806), tensor(5.1381), tensor(4.9563), tensor(4.8918), tensor(4.8024), tensor(4.8304), tensor(5.2082), tensor(5.0258), tensor(5.0727), tensor(5.1227), tensor(4.9057), tensor(4.8960), tensor(4.8588), tensor(5.1716), tensor(5.1688), tensor(4.9531), tensor(5.0655), tensor(4.9093), tensor(4.9689), tensor(4.9919), tensor(4.9802), tensor(5.0626), tensor(5.0606), tensor(4.6610), tensor(4.9523), tensor(4.9201), tensor(4.9721), tensor(5.1272), tensor(5.0233), tensor(5.1137), tensor(5.0153), tensor(4.9872), tensor(4.9189), tensor(5.0017), tensor(4.9830), tensor(4.8515), tensor(4.9191), tensor(5.0226), tensor(5.0212), tensor(5.1113), tensor(5.4241), tensor(5.2029), tensor(5.0763), tensor(5.1229), tensor(5.0432), tensor(4.9525), tensor(5.0229), tensor(4.6932), tensor(4.8810), tensor(5.0262), tensor(5.0160), tensor(5.0452), tensor(4.9813), tensor(5.0039), tensor(4.9057), tensor(4.7157), tensor(4.8962), tensor(5.0142), tensor(4.9451), tensor(4.9844), tensor(4.8461), tensor(4.9371), tensor(4.8162), tensor(4.8810), tensor(4.6881), tensor(4.6476), tensor(4.9460), tensor(4.8981), tensor(5.0563), tensor(4.9403), tensor(4.9732), tensor(4.9089), tensor(5.0414), tensor(4.9943), tensor(4.8844), tensor(4.9162), tensor(5.0146), tensor(4.9934), tensor(4.9827), tensor(5.0096), tensor(5.0112), tensor(5.0294), tensor(4.9706), tensor(5.1039), tensor(5.1553), tensor(5.2302), tensor(4.7567), tensor(4.9984), tensor(4.9776), tensor(5.1653), tensor(5.0728), tensor(4.9941), tensor(4.9628), tensor(5.0756), tensor(5.0256), tensor(5.3380), tensor(5.0493), tensor(5.2061), tensor(5.1107), tensor(5.1252), tensor(5.1979), tensor(5.0961), tensor(5.0820), tensor(5.0030), tensor(4.9694), tensor(5.0283), tensor(5.0448), tensor(5.0435), tensor(5.1160), tensor(5.0811), tensor(4.9488), tensor(5.1035), tensor(4.9932), tensor(4.9909), tensor(4.8847), tensor(4.9096), tensor(4.9433), tensor(4.9253), tensor(5.1573), tensor(5.0462), tensor(4.9816), tensor(5.1988), tensor(5.0127), tensor(5.0377), tensor(5.0589), tensor(5.0072), tensor(5.0459), tensor(5.0998), tensor(4.9870), tensor(5.0419), tensor(5.1126), tensor(5.1082), tensor(5.1482), tensor(5.2123), tensor(5.0963), tensor(5.1296), tensor(5.0750), tensor(5.3276), tensor(5.0516), tensor(4.9788), tensor(4.9253), tensor(5.5747), tensor(5.1228), tensor(5.1117), tensor(5.1109), tensor(5.1466), tensor(5.1797), tensor(5.0075), tensor(5.0419), tensor(4.9676), tensor(4.9702), tensor(4.9955), tensor(4.9880), tensor(5.0206), tensor(4.9802), tensor(4.8194), tensor(4.9831), tensor(4.8407), tensor(4.7406), tensor(4.9042), tensor(4.8862), tensor(5.0223), tensor(4.9531), tensor(4.9472), tensor(4.8349), tensor(4.9591), tensor(4.8850), tensor(4.9306), tensor(4.9194), tensor(5.0108), tensor(4.8904), tensor(4.9131), tensor(5.0351), tensor(4.9527), tensor(5.0099), tensor(5.0477), tensor(5.0840), tensor(5.4918), tensor(5.4287), tensor(5.3054), tensor(5.1275), tensor(4.9038), tensor(5.0525), tensor(5.0963), tensor(4.9093), tensor(4.9818), tensor(4.9926), tensor(5.3252), tensor(5.0184), tensor(4.8002), tensor(4.9742), tensor(5.0030), tensor(5.0870), tensor(4.9720), tensor(4.9483), tensor(5.0493), tensor(5.0137), tensor(5.0248), tensor(5.0208), tensor(4.9939), tensor(5.0786), tensor(5.2020), tensor(5.2411), tensor(5.0464), tensor(5.0273), tensor(4.8638), tensor(5.0060), tensor(4.9887), tensor(4.8834), tensor(4.8389), tensor(4.9932), tensor(4.9702), tensor(4.9293), tensor(4.9924), tensor(5.0161), tensor(4.9585), tensor(4.9208), tensor(4.9027), tensor(5.0073), tensor(5.3116), tensor(5.1227), tensor(4.9142), tensor(4.9553), tensor(5.0734), tensor(5.0567), tensor(4.9475), tensor(5.4018), tensor(5.1277), tensor(5.1832), tensor(4.9450), tensor(4.9611), tensor(5.0444), tensor(4.9856), tensor(5.1027), tensor(4.9804), tensor(4.8688), tensor(5.0003), tensor(4.9590), tensor(5.0714), tensor(5.0364), tensor(5.0022), tensor(5.0260), tensor(5.1667), tensor(5.1262), tensor(4.9563), tensor(5.0640), tensor(5.0393), tensor(4.9611), tensor(5.4720), tensor(5.2137), tensor(5.4359), tensor(5.1287), tensor(5.1185), tensor(4.9246), tensor(4.8491), tensor(4.7854), tensor(4.9067), tensor(4.9268), tensor(5.0125), tensor(4.8947), tensor(4.9991), tensor(5.0079), tensor(4.9740), tensor(4.9337), tensor(5.0180), tensor(4.9934), tensor(4.8810), tensor(4.9884), tensor(4.9081), tensor(4.9401), tensor(5.0952), tensor(5.0691), tensor(5.0030), tensor(5.0596), tensor(5.2760), tensor(5.1638), tensor(5.0407), tensor(4.9753), tensor(4.9519), tensor(4.7669), tensor(4.9887), tensor(4.7700), tensor(4.9509), tensor(4.8693), tensor(4.9366), tensor(4.9088), tensor(5.0276), tensor(4.9673), tensor(4.8825), tensor(4.7000), tensor(4.8701), tensor(4.9007), tensor(4.8642), tensor(5.0080), tensor(4.9869), tensor(4.9676), tensor(4.9352), tensor(5.0301), tensor(4.9584), tensor(4.9657), tensor(4.8177), tensor(4.8871), tensor(5.1196), tensor(5.1286), tensor(4.9815), tensor(5.0102), tensor(5.1274), tensor(4.9444), tensor(5.0771), tensor(4.9133), tensor(4.8513), tensor(4.8852), tensor(5.0499), tensor(5.0746), tensor(5.1037), tensor(4.9833), tensor(4.8723), tensor(4.8932), tensor(4.9310), tensor(4.9678), tensor(4.9872), tensor(4.9517), tensor(4.8672), tensor(4.9553), tensor(5.0674), tensor(4.9597), tensor(4.9304), tensor(4.9029), tensor(4.9963), tensor(5.0598), tensor(5.0187), tensor(5.0629), tensor(5.0744), tensor(5.0980), tensor(5.0344), tensor(5.1581), tensor(4.9450), tensor(5.0042), tensor(5.0773), tensor(4.9447), tensor(4.9694), tensor(5.0060), tensor(5.1010), tensor(5.1414), tensor(4.9921), tensor(5.0425), tensor(5.1835), tensor(4.9969), tensor(5.0116), tensor(5.0312), tensor(4.9853), tensor(4.9794), tensor(5.0027), tensor(4.9236), tensor(4.9850), tensor(5.0500), tensor(4.8752), tensor(4.9582), tensor(4.9943), tensor(4.9606), tensor(4.9905), tensor(4.7879), tensor(4.9399), tensor(5.0088), tensor(4.9360), tensor(4.9592), tensor(4.9499), tensor(5.1024), tensor(5.1277), tensor(5.1228), tensor(5.0755), tensor(5.0466), tensor(4.9454), tensor(5.0215), tensor(5.0003), tensor(4.9912), tensor(4.9153), tensor(4.9880), tensor(4.9999), tensor(5.1316), tensor(5.0712), tensor(5.0463), tensor(5.0534), tensor(4.9746), tensor(4.9898), tensor(5.5664), tensor(5.2360), tensor(4.8796), tensor(4.8980), tensor(4.8683), tensor(4.8671), tensor(4.9828), tensor(5.0554), tensor(4.8851), tensor(4.9367), tensor(4.9398), tensor(4.9752), tensor(4.5854), tensor(4.9514), tensor(4.9180), tensor(4.9834), tensor(5.0206), tensor(5.1434), tensor(5.0620), tensor(4.9865), tensor(4.7030), tensor(4.8520), tensor(5.0084), tensor(5.0670), tensor(4.9790), tensor(5.0698), tensor(5.0921), tensor(5.1562), tensor(5.0828), tensor(4.9841), tensor(5.0264), tensor(4.9766), tensor(5.0313), tensor(4.8874), tensor(4.8536), tensor(4.9318), tensor(4.8003), tensor(4.9817), tensor(4.7898), tensor(5.0668), tensor(4.8857), tensor(4.9922), tensor(4.9711), tensor(4.9702), tensor(5.0397), tensor(5.0479), tensor(5.1012), tensor(5.0009), tensor(5.0185), tensor(5.0178), tensor(5.0523), tensor(5.0911), tensor(5.1100), tensor(4.9240), tensor(4.9295), tensor(4.9595), tensor(4.9016), tensor(4.9443), tensor(5.0408), tensor(5.0204), tensor(5.1513), tensor(4.9263), tensor(4.9965), tensor(5.0754), tensor(4.6883), tensor(4.7986), tensor(4.9790), tensor(5.0706), tensor(5.3589), tensor(5.2096), tensor(5.1490), tensor(4.9452), tensor(5.3227), tensor(5.0477), tensor(4.9882), tensor(4.9232), tensor(4.9853), tensor(4.9772), tensor(5.0547), tensor(4.8680), tensor(4.9279), tensor(4.9746), tensor(4.9042), tensor(5.0156), tensor(5.0527), tensor(5.0371), tensor(5.1167), tensor(4.9270), tensor(5.2503), tensor(5.0034), tensor(5.1486), tensor(5.0984), tensor(5.1248), tensor(5.0880), tensor(4.9608), tensor(4.8960), tensor(5.2096), tensor(5.0265), tensor(4.9403), tensor(5.1545), tensor(5.1360), tensor(5.1509), tensor(5.0161), tensor(5.1268), tensor(5.0673)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "\n",
    "class SMC():\n",
    "    def __init__(self, n_threads: int, n_iterations: int):\n",
    "        self.n_threads = n_threads\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta = [self.initial_theta(loc=0, scale=1)]\n",
    "        self.weights = [self.initial_weights()]\n",
    "        self.estimate = []\n",
    "        print(f'smc initialised, use run_smc to run all iterations')\n",
    "        return\n",
    "    \n",
    "    def target_dist_prob(self, th):\n",
    "        normal = Normal(torch.tensor([5]),torch.tensor([0.5]))\n",
    "        return torch.exp(normal.log_prob(th))\n",
    "\n",
    "    def initial_weights(self):\n",
    "        normal = Normal(torch.tensor([0]),torch.tensor([1]))\n",
    "        # maybe this is stupid but idk why theres no way of getting prob\n",
    "        # only thing that i can get returned is log prob instead\n",
    "        prob_q = (torch.exp(normal.log_prob(self.theta[-1])))\n",
    "\n",
    "        prob_target = self.target_dist_prob(self.theta[-1])\n",
    "   \n",
    "        return prob_target/prob_q\n",
    "    \n",
    "    def initial_theta(self, loc, scale):\n",
    "        # maybe this is stupid but idk why theres no\n",
    "        return torch.normal(loc, scale,size=(1,self.n_threads))[-1]\n",
    "    \n",
    "    def normalise_weights_(self):\n",
    "        self.weights[-1] = self.weights[-1] / self.weights[-1].sum()\n",
    "    \n",
    "    def calc_neff(self):\n",
    "        return 1 / (self.weights[-1] @ self.weights[-1].T)\n",
    "    \n",
    "    def new_weights_(self):\n",
    "        target_new = self.target_dist_prob(self.theta[-1])\n",
    "        target_old = self.target_dist_prob(self.theta[-2])\n",
    "        self.weights.append(self.weights[-1] * (target_new/target_old))\n",
    "        return\n",
    "    \n",
    "    def weighted_resample_(self):\n",
    "        r\"\"\"\n",
    "        in place chages theta and weights using a weighted resample\n",
    "        \"\"\"\n",
    "        new_sample_indexes = torch.multinomial(self.weights[-1],self.n_threads,replacement=True)\n",
    "\n",
    "        theta_hat = torch.tensor([self.theta[-1][i] for i in new_sample_indexes])\n",
    " \n",
    "        self.theta[-1] = theta_hat\n",
    "        self.weights[-1] = (torch.ones(1,self.n_threads)[-1])*(1/self.n_threads)\n",
    "\n",
    "    def estimator_(self):\n",
    "        self.estimate.append(self.theta[-1] @ self.weights[-1].T)\n",
    "\n",
    "    def sample_(self):\n",
    "        self.theta.append(self.theta[-1]+torch.normal(0,1,size=(1,self.n_threads))[-1])\n",
    "\n",
    "    def run_smc(self):\n",
    "        for _ in range(self.n_iterations):\n",
    "            self.normalise_weights_()\n",
    "            # estimate quals of interest\n",
    "            self.estimator_()\n",
    "\n",
    "            # assess if we need to resample\n",
    "            neff = self.calc_neff()\n",
    "            if neff<(self.n_threads/2):\n",
    "                self.weighted_resample_()\n",
    "            self.sample_()\n",
    "            self.new_weights_()\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    n = 100\n",
    "    t = 500\n",
    "    smc_torch = SMC(n,t)\n",
    "    smc_torch.run_smc()\n",
    "    print(smc_torch.estimate)\n",
    "    return\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
